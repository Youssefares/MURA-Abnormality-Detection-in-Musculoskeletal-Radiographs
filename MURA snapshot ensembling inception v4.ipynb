{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from inception import inception_v3\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    " \n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t\t       train_labeled_studies.csv  valid_image_paths.csv\r\n",
      "train_image_paths.csv  valid\t\t\t  valid_labeled_studies.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls MURA-v1.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MURA-v1.1/valid/XR_WRIST/patient11186/study1_p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...\n",
       "1  MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...\n",
       "2  MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...\n",
       "3  MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...\n",
       "4  MURA-v1.1/valid/XR_WRIST/patient11186/study1_p..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labled_studies = pd.read_csv('MURA-v1.1/valid_image_paths.csv', header=None)\n",
    "train_labled_studies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MURA-v1.1/valid/XR_WRIST/patient11185/study1_positive/image1.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labled_studies.loc[0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuraDatasetByStudy(Dataset):\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(csv_file, header=None)\n",
    "        self.study_pths = df.iloc[:,0]\n",
    "        self.labels = df.iloc[:, 1]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_pths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        study_path = self.study_pths[idx]\n",
    "        \n",
    "        # get images for this study\n",
    "        i = 0\n",
    "        images = []\n",
    "        while(True):\n",
    "            img_name = study_path + 'image%s.png' % (i+1)\n",
    "            try:\n",
    "                img = Image.open(img_name)\n",
    "                img = img.convert('RGB')\n",
    "                images.append(self.transform(img))\n",
    "                i += 1\n",
    "            except FileNotFoundError:\n",
    "                break\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        m = re.match(r'.*/XR_(\\w*)/patient(\\d+)/study(\\d+)_', img_name)\n",
    "        study_type = m.group(1)\n",
    "        study_id = m.group(2)+'/'+m.group(3)\n",
    "        \n",
    "        return images, study_type, study_id, label\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx >= len(self):\n",
    "            raise StopIteration\n",
    "        \n",
    "        result = self[self.idx]\n",
    "        self.idx += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuraDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform, augment_transforms=[]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        traindf = pd.read_csv(csv_file, header=None)\n",
    "        self.train_img_pths = traindf.iloc[:,0]\n",
    "        self.transform = transform\n",
    "        self.augment_transforms = augment_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.augment_transforms)+1)*len(self.train_img_pths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.train_img_pths):\n",
    "            idx = idx%len(self.train_img_pths)\n",
    "            transform = self.augment_transforms[idx//len(self.train_img_pths)-0]\n",
    "        else:\n",
    "            transform = self.transform\n",
    "\n",
    "        img_name = self.train_img_pths[idx]\n",
    "        img = Image.open(img_name)\n",
    "        img = img.convert('RGB')\n",
    "        img = transform(img)\n",
    "        \n",
    "        img_study_type = re.match(r'.*/XR_(\\w*)/', img_name).group(1)\n",
    "        img_label = int(\n",
    "            re.match(r'.*/study\\d+_(\\w+)/', img_name).group(1) == 'positive'\n",
    "        )\n",
    "        img_study_name = re.match(r'(.*/).*png', img_name).group(1)\n",
    "        \n",
    "        return img, img_study_name, img_study_type, img_label\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx >= len(self):\n",
    "            raise StopIteration\n",
    "        \n",
    "        result = self[self.idx]\n",
    "        self.idx += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_counts = pd.read_csv('study_counts.csv')\n",
    "study_counts = study_counts.set_index('study_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(study_type):\n",
    "    Nt = study_counts.loc[study_type].loc['label_positive']\n",
    "    At = study_counts.loc[study_type].loc['label_negative']\n",
    "    return At/(At+Nt), Nt/(At+Nt), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ELBOW': (0.593185966335429, 0.4068140336645711),\n",
       " 'FINGER': (0.6145710928319624, 0.3854289071680376),\n",
       " 'FOREARM': (0.6378082191780822, 0.36219178082191783),\n",
       " 'HAND': (0.7322749413674905, 0.26772505863250945),\n",
       " 'HUMERUS': (0.5290880503144654, 0.47091194968553457),\n",
       " 'SHOULDER': (0.5025659386561642, 0.4974340613438358),\n",
       " 'WRIST': (0.591160787530763, 0.4088392124692371)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wt = {t : weights(t) for t in study_counts.index}\n",
    "Wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(torch.nn.modules.Module):\n",
    "    def __init__(self, W, cpu=False):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.W = W\n",
    "        self.T = torch.DoubleTensor if cpu else torch.cuda.DoubleTensor\n",
    "    \n",
    "    def forward(self, inputs, targets, keys):\n",
    "        Wt1 = self.T([self.W[key][1] for key in keys])\n",
    "        Wt0 = self.T([self.W[key][0] for key in keys])\n",
    "        targets = targets.double()\n",
    "        inputs = inputs.double()\n",
    "        print(inputs)\n",
    "        print(inputs.log())\n",
    "#         loss = - (Wt1*targets*inputs.log() + Wt0*(1-targets)*(1-inputs).log())\n",
    "        return F.binary_cross_entropy(inputs, targets, weight=Wt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del study_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "model = inception_v3(pretrained=False, num_classes=1)\n",
    "# freeze everything to extract feature vector\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Handle the auxilary net\n",
    "num_ftrs = model.AuxLogits.fc.in_features\n",
    "model.AuxLogits.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# Handle the primary net\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (299, 299)\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "augment_transforms = [\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "\n",
    "datasets_dict = {\n",
    "    x: MuraDataset(\n",
    "        csv_file='MURA-v1.1/%s_image_paths.csv' % x,\n",
    "        transform=data_transforms[x],\n",
    "        augment_transforms=augment_transforms if x == 'train' else [],\n",
    "    ) for x in ['train', 'valid']\n",
    "}\n",
    "\n",
    "# # subset\n",
    "# dataloaders_dict = {\n",
    "#     x: DataLoader(\n",
    "#         torch.utils.data.Subset(\n",
    "#             dataset=datasets_dict[x],\n",
    "#             indices=np.random.randint(\n",
    "#                 len(datasets_dict[x]),\n",
    "#                 size=100,\n",
    "#             ),\n",
    "#         ),\n",
    "#         batch_size=batch_size,\n",
    "#         num_workers=8,\n",
    "#     ) for x in ['train', 'valid']\n",
    "# }\n",
    "\n",
    "dataloaders_dict = {\n",
    "    x: DataLoader(\n",
    "        datasets_dict[x],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=8,\n",
    "    ) for x in ['train', 'valid']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(optimizer, update):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = update(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 8\n",
    "T = batch_size*epoch_n\n",
    "M = 5\n",
    "\n",
    "def shifted_cosine_function(t, lr):\n",
    "    return lr/2*(np.cos(np.pi*((t-1)%np.ceil(T/M))/np.ceil(T/M))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epoch_n, cycles, validation_loader):\n",
    "    model = model.to(device)\n",
    "    t = 0 # Total number of iterations\n",
    "    models = []\n",
    "    for cycle in range(cycles):\n",
    "        # reset learning rate\n",
    "        update_learning_rate(optimizer, lambda lr: 0.2)\n",
    "        for epoch in range(epoch_n):\n",
    "            model.train()\n",
    "            print('=========================epoch %i=====================' % epoch)\n",
    "            epoch_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            for i, data in enumerate(tqdm(train_loader)):\n",
    "                # update learning rate\n",
    "                t += 1\n",
    "                update_learning_rate(optimizer, lambda lr: shifted_cosine_function(t, lr))\n",
    "\n",
    "                inputs, _, study_type, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                Wt1 = torch.FloatTensor([Wt[key][1] for key in study_type])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs, aux_outputs = model(inputs)\n",
    "                outputs = torch.mean(outputs, 1)\n",
    "                aux_outputs = torch.mean(aux_outputs, 1)\n",
    "                loss1 = criterion(outputs, labels.float())\n",
    "                loss2 = criterion(aux_outputs, labels.float())\n",
    "                loss = loss1 + 0.4*loss2\n",
    "                \n",
    "                loss.sum().backward()\n",
    "                optimizer.step()\n",
    "                preds = (outputs > 0.5).type(torch.cuda.LongTensor)\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.sum()\n",
    "                epoch_loss += loss.sum()\n",
    "\n",
    "            epoch_loss = epoch_loss / len(train_loader.dataset)\n",
    "            epoch_accuracy = epoch_corrects.double() / len(train_loader.dataset)\n",
    "            print('epoch loss: %.8f' % epoch_loss)\n",
    "            print('epoch accuracy: %.8f' % epoch_accuracy)\n",
    "\n",
    "            # else evaluate first then keep training\n",
    "            _, valid_loss, valid_acc, valid_kappa = validate_model(\n",
    "                [model],\n",
    "                validation_loader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "            )\n",
    "            print('epoch valid_loss: %8f' % valid_loss)\n",
    "            print('epoch valid_acc: %.8f' % valid_acc)\n",
    "            print('epoch valid_kappa: %.8f' % valid_kappa)\n",
    "        \n",
    "        print('cycle %i is over.' % cycle)\n",
    "        print('saving model..\\n')\n",
    "        models.append(model.state_dict())\n",
    "    return models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(models, validation_loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    returns (df, acc, kappa)\n",
    "        df: Pandas dataframe containing study paths and their corresponding label\n",
    "        obtained by averaging the model scores on the study's images and treating any\n",
    "        score higher than 0.5 as abnormal.\n",
    "        \n",
    "        acc: model accuracy score on studies\n",
    "        kappa: model kappa score on studies\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    all_study_names = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(validation_loader)):\n",
    "        inputs, study_names, study_types, labels = data\n",
    "        all_study_names += study_names\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            ensembled_outputs = []\n",
    "            for model in models:\n",
    "                outputs = model(inputs)\n",
    "                outputs = torch.mean(outputs, 1)\n",
    "                ensembled_outputs.append(outputs)\n",
    "            outputs = torch.stack(ensembled_outputs)\n",
    "            outputs = torch.mean(outputs, 0)\n",
    "            all_outputs += outputs.tolist()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        total_loss += loss.sum()\n",
    "    \n",
    "    total_loss /= len(validation_loader.dataset)\n",
    "    results_df = pd.DataFrame({\n",
    "        'study': all_study_names,\n",
    "        'prob': all_outputs,\n",
    "    })\n",
    "    \n",
    "    predicted_labeled_studies = results_df.groupby('study').agg({'prob': 'mean'})\n",
    "    predicted_labeled_studies = predicted_labeled_studies.reset_index()\n",
    "    \n",
    "    predicted_labeled_studies['prediction'] = (predicted_labeled_studies['prob'] > 0.5).astype('int8')\n",
    "    \n",
    "    valid_labeled_studies = pd.read_csv(\n",
    "        'MURA-v1.1/valid_labeled_studies.csv',\n",
    "        header=None,\n",
    "        names= ['study', 'label']\n",
    "    )\n",
    "    \n",
    "   \n",
    "    joined_df = predicted_labeled_studies.join(valid_labeled_studies.set_index('study'), on='study')\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(joined_df.label, joined_df.prediction)\n",
    "    kappa = cohen_kappa_score(joined_df.label, joined_df.prediction)\n",
    "    \n",
    "    return joined_df, total_loss.item(), accuracy, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================epoch 0=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 945/9202 [05:55<51:54,  2.65it/s] "
     ]
    }
   ],
   "source": [
    "best_models = train_model(model, dataloaders_dict['train'], criterion, optimizer, epoch_n, M, dataloaders_dict['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(best_models, prefix=''):\n",
    "    \"\"\"\n",
    "    best_models: [(valid_acc, model_state_dict)]\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for i, model in enumerate(best_models):\n",
    "        path = 'models/'+prefix+'model_'+str(i)\n",
    "        paths.append(path)\n",
    "        # save model_state_dict\n",
    "        torch.save(model, path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(paths):\n",
    "    models = []\n",
    "    for path in paths:\n",
    "        m = inception_v3(pretrained=False, num_classes=1)\n",
    "        m.load_state_dict(torch.load(path))\n",
    "        models.append(m)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = save_models(best_models, prefix='snapshot_ensembling_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch valid_loss: 1.381551\n",
      "epoch valid_acc: 0.61855670\n",
      "epoch valid_kappa: 0.16164448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, valid_loss, valid_acc, valid_kappa = validate_model(\n",
    "        models,\n",
    "        dataloaders_dict['valid'],\n",
    "        criterion,\n",
    "        optimizer,\n",
    "    )\n",
    "\n",
    "print('epoch valid_loss: %8f' % valid_loss)\n",
    "print('epoch valid_acc: %.8f' % valid_acc)\n",
    "print('epoch valid_kappa: %.8f' % valid_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
